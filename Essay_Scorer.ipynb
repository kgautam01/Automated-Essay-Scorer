{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux-device/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/linux-device/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing required packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "importing_dataset = pd.read_csv('training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
    "\n",
    "# dependent variable\n",
    "scores = importing_dataset['domain1_score']\n",
    "dataset = importing_dataset.loc[:,['essay_id', 'essay_set', 'essay', 'domain1_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating word tokens after removing characters other than alphabets, converting them to lower case and\n",
    "# removing stopwords from the text'''\n",
    "\n",
    "def word_tokens(essay_text):\n",
    "    essay_text = re.sub(\"[^a-zA-Z]\", \" \", essay_text)\n",
    "    words = essay_text.lower().split()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    return (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating sentence tokens from the essay and finally the word tokens\n",
    "\n",
    "def sentence_tokens(essay_text):\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sent_tokens = tokenizer.tokenize(essay_text.strip())\n",
    "    sentences = []\n",
    "    for sent_token in sent_tokens:\n",
    "        if len(sent_token) > 0:\n",
    "            sentences.append(word_tokens(sent_token))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a vector of features\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model[word])        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating word vectors to be used in word2vec model\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay_text in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay_text, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------Fold 1------------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux-device/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10380/10380 [==============================] - 7s 649us/step - loss: 63.7772 - mean_absolute_error: 4.3342\n",
      "Epoch 2/50\n",
      "10380/10380 [==============================] - 5s 467us/step - loss: 39.4171 - mean_absolute_error: 3.4872\n",
      "Epoch 3/50\n",
      "10380/10380 [==============================] - 5s 464us/step - loss: 33.6297 - mean_absolute_error: 3.4191\n",
      "Epoch 4/50\n",
      "10380/10380 [==============================] - 5s 465us/step - loss: 31.1998 - mean_absolute_error: 3.3913\n",
      "Epoch 5/50\n",
      "10380/10380 [==============================] - 5s 464us/step - loss: 29.4576 - mean_absolute_error: 3.3128\n",
      "Epoch 6/50\n",
      "10380/10380 [==============================] - 5s 469us/step - loss: 28.2531 - mean_absolute_error: 3.2038\n",
      "Epoch 7/50\n",
      "10380/10380 [==============================] - 5s 464us/step - loss: 26.6130 - mean_absolute_error: 3.0895\n",
      "Epoch 8/50\n",
      "10380/10380 [==============================] - 5s 468us/step - loss: 25.3848 - mean_absolute_error: 2.9354\n",
      "Epoch 9/50\n",
      "10380/10380 [==============================] - 5s 468us/step - loss: 22.4176 - mean_absolute_error: 2.7214\n",
      "Epoch 10/50\n",
      "10380/10380 [==============================] - 5s 468us/step - loss: 20.0745 - mean_absolute_error: 2.5880\n",
      "Epoch 11/50\n",
      "10380/10380 [==============================] - 5s 467us/step - loss: 17.6091 - mean_absolute_error: 2.4513\n",
      "Epoch 12/50\n",
      "10380/10380 [==============================] - 5s 469us/step - loss: 15.7893 - mean_absolute_error: 2.3178\n",
      "Epoch 13/50\n",
      "10380/10380 [==============================] - 5s 465us/step - loss: 15.1767 - mean_absolute_error: 2.2529\n",
      "Epoch 14/50\n",
      "10380/10380 [==============================] - 5s 472us/step - loss: 14.4079 - mean_absolute_error: 2.2143\n",
      "Epoch 15/50\n",
      "10380/10380 [==============================] - 5s 472us/step - loss: 13.6730 - mean_absolute_error: 2.1211\n",
      "Epoch 16/50\n",
      "10380/10380 [==============================] - 5s 471us/step - loss: 13.1668 - mean_absolute_error: 2.0807\n",
      "Epoch 17/50\n",
      "10380/10380 [==============================] - 5s 475us/step - loss: 12.6614 - mean_absolute_error: 2.0481\n",
      "Epoch 18/50\n",
      "10380/10380 [==============================] - 5s 472us/step - loss: 12.5401 - mean_absolute_error: 2.0224\n",
      "Epoch 19/50\n",
      "10380/10380 [==============================] - 5s 475us/step - loss: 12.2538 - mean_absolute_error: 2.0178\n",
      "Epoch 20/50\n",
      "10380/10380 [==============================] - 5s 472us/step - loss: 11.4010 - mean_absolute_error: 1.9358\n",
      "Epoch 21/50\n",
      "10380/10380 [==============================] - 5s 473us/step - loss: 11.3805 - mean_absolute_error: 1.9347\n",
      "Epoch 22/50\n",
      "10380/10380 [==============================] - 5s 477us/step - loss: 11.4834 - mean_absolute_error: 1.9127\n",
      "Epoch 23/50\n",
      "10380/10380 [==============================] - 5s 486us/step - loss: 11.1008 - mean_absolute_error: 1.8997\n",
      "Epoch 24/50\n",
      "10380/10380 [==============================] - 5s 495us/step - loss: 10.4986 - mean_absolute_error: 1.8507\n",
      "Epoch 25/50\n",
      "10380/10380 [==============================] - 5s 473us/step - loss: 10.3693 - mean_absolute_error: 1.8173\n",
      "Epoch 26/50\n",
      "10380/10380 [==============================] - 5s 481us/step - loss: 9.8137 - mean_absolute_error: 1.7993\n",
      "Epoch 27/50\n",
      "10380/10380 [==============================] - 5s 525us/step - loss: 9.7290 - mean_absolute_error: 1.7729\n",
      "Epoch 28/50\n",
      "10380/10380 [==============================] - 5s 512us/step - loss: 9.6396 - mean_absolute_error: 1.7629\n",
      "Epoch 29/50\n",
      "10380/10380 [==============================] - 5s 518us/step - loss: 9.6943 - mean_absolute_error: 1.7465\n",
      "Epoch 30/50\n",
      "10380/10380 [==============================] - 5s 497us/step - loss: 9.2689 - mean_absolute_error: 1.7300\n",
      "Epoch 31/50\n",
      "10380/10380 [==============================] - 5s 451us/step - loss: 9.3726 - mean_absolute_error: 1.7273\n",
      "Epoch 32/50\n",
      "10380/10380 [==============================] - 5s 446us/step - loss: 9.6815 - mean_absolute_error: 1.7152\n",
      "Epoch 33/50\n",
      "10380/10380 [==============================] - 5s 442us/step - loss: 9.4815 - mean_absolute_error: 1.7178\n",
      "Epoch 34/50\n",
      "10380/10380 [==============================] - 5s 498us/step - loss: 8.9051 - mean_absolute_error: 1.6924\n",
      "Epoch 35/50\n",
      "10380/10380 [==============================] - 5s 488us/step - loss: 8.9079 - mean_absolute_error: 1.6657\n",
      "Epoch 36/50\n",
      "10380/10380 [==============================] - 5s 467us/step - loss: 8.8306 - mean_absolute_error: 1.6545\n",
      "Epoch 37/50\n",
      "10380/10380 [==============================] - 5s 469us/step - loss: 8.6551 - mean_absolute_error: 1.6528\n",
      "Epoch 38/50\n",
      "10380/10380 [==============================] - 5s 479us/step - loss: 9.1552 - mean_absolute_error: 1.6861\n",
      "Epoch 39/50\n",
      "10380/10380 [==============================] - 5s 468us/step - loss: 8.9974 - mean_absolute_error: 1.6519\n",
      "Epoch 40/50\n",
      "10380/10380 [==============================] - 5s 473us/step - loss: 8.3656 - mean_absolute_error: 1.6257\n",
      "Epoch 41/50\n",
      "10380/10380 [==============================] - 5s 468us/step - loss: 8.9032 - mean_absolute_error: 1.6436\n",
      "Epoch 42/50\n",
      "10380/10380 [==============================] - 5s 473us/step - loss: 8.4634 - mean_absolute_error: 1.6257\n",
      "Epoch 43/50\n",
      "10380/10380 [==============================] - 5s 469us/step - loss: 8.3520 - mean_absolute_error: 1.6138\n",
      "Epoch 44/50\n",
      "10380/10380 [==============================] - 5s 470us/step - loss: 8.3539 - mean_absolute_error: 1.6116\n",
      "Epoch 45/50\n",
      "10380/10380 [==============================] - 5s 475us/step - loss: 8.1254 - mean_absolute_error: 1.5991\n",
      "Epoch 46/50\n",
      "10380/10380 [==============================] - 5s 475us/step - loss: 8.0194 - mean_absolute_error: 1.5896\n",
      "Epoch 47/50\n",
      "10380/10380 [==============================] - 5s 478us/step - loss: 8.4264 - mean_absolute_error: 1.6055\n",
      "Epoch 48/50\n",
      "10380/10380 [==============================] - 5s 481us/step - loss: 8.4931 - mean_absolute_error: 1.5901\n",
      "Epoch 49/50\n",
      "10380/10380 [==============================] - 6s 548us/step - loss: 7.9937 - mean_absolute_error: 1.5764\n",
      "Epoch 50/50\n",
      "10380/10380 [==============================] - 5s 481us/step - loss: 8.1566 - mean_absolute_error: 1.5717\n",
      "Mean squared error: 5.60\n",
      "Variance: 0.93\n",
      "Kappa Score: 0.96\n",
      "\n",
      "------------Fold 2------------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 7s 692us/step - loss: 62.6865 - mean_absolute_error: 4.3106\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 38.6122 - mean_absolute_error: 3.4851\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 5s 487us/step - loss: 32.7122 - mean_absolute_error: 3.4102\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10381/10381 [==============================] - 5s 446us/step - loss: 29.7156 - mean_absolute_error: 3.3266\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 5s 449us/step - loss: 28.4775 - mean_absolute_error: 3.2620\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 5s 447us/step - loss: 27.8784 - mean_absolute_error: 3.1805\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 5s 451us/step - loss: 25.7029 - mean_absolute_error: 3.0430\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 5s 452us/step - loss: 24.7546 - mean_absolute_error: 2.8816\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 5s 450us/step - loss: 21.6947 - mean_absolute_error: 2.7223\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 5s 453us/step - loss: 19.4678 - mean_absolute_error: 2.5736\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 5s 453us/step - loss: 18.0379 - mean_absolute_error: 2.4498\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 5s 458us/step - loss: 16.7294 - mean_absolute_error: 2.3510\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 5s 457us/step - loss: 16.1059 - mean_absolute_error: 2.2935\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 5s 453us/step - loss: 15.3065 - mean_absolute_error: 2.2370\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 5s 455us/step - loss: 14.2939 - mean_absolute_error: 2.1639\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 5s 461us/step - loss: 13.9246 - mean_absolute_error: 2.1221\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 5s 459us/step - loss: 13.5832 - mean_absolute_error: 2.1157\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 5s 461us/step - loss: 14.0736 - mean_absolute_error: 2.1194\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 5s 459us/step - loss: 12.9440 - mean_absolute_error: 2.0438\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 5s 462us/step - loss: 12.5649 - mean_absolute_error: 2.0161\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 5s 460us/step - loss: 12.4063 - mean_absolute_error: 1.9689\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 5s 461us/step - loss: 11.5690 - mean_absolute_error: 1.9284\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 5s 460us/step - loss: 11.3421 - mean_absolute_error: 1.9170\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 5s 460us/step - loss: 11.3391 - mean_absolute_error: 1.8872\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 5s 463us/step - loss: 11.0311 - mean_absolute_error: 1.8650\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 5s 464us/step - loss: 10.6808 - mean_absolute_error: 1.8235\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 5s 461us/step - loss: 10.0857 - mean_absolute_error: 1.7931\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 5s 464us/step - loss: 10.6067 - mean_absolute_error: 1.8205\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 5s 469us/step - loss: 10.3856 - mean_absolute_error: 1.7815\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 5s 466us/step - loss: 10.3642 - mean_absolute_error: 1.7707\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 5s 462us/step - loss: 9.9882 - mean_absolute_error: 1.7555\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 5s 466us/step - loss: 9.6904 - mean_absolute_error: 1.7350\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 5s 464us/step - loss: 9.7438 - mean_absolute_error: 1.7224\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 5s 466us/step - loss: 9.5927 - mean_absolute_error: 1.7303\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 5s 462us/step - loss: 9.3171 - mean_absolute_error: 1.6993\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 5s 468us/step - loss: 9.0517 - mean_absolute_error: 1.6927\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 5s 465us/step - loss: 9.8547 - mean_absolute_error: 1.7207\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 5s 470us/step - loss: 9.3068 - mean_absolute_error: 1.6920\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 6s 536us/step - loss: 9.2804 - mean_absolute_error: 1.6669\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 5s 487us/step - loss: 9.3777 - mean_absolute_error: 1.6696\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 5s 484us/step - loss: 8.9424 - mean_absolute_error: 1.6698\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 5s 490us/step - loss: 8.8538 - mean_absolute_error: 1.6635\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 5s 488us/step - loss: 8.7728 - mean_absolute_error: 1.6391\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 5s 489us/step - loss: 8.7101 - mean_absolute_error: 1.6376\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 5s 488us/step - loss: 8.9361 - mean_absolute_error: 1.6463\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 5s 491us/step - loss: 8.6149 - mean_absolute_error: 1.6334\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 5s 489us/step - loss: 8.2909 - mean_absolute_error: 1.6135\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 5s 490us/step - loss: 8.8053 - mean_absolute_error: 1.6279\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 8.3938 - mean_absolute_error: 1.5924\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 5s 509us/step - loss: 8.6121 - mean_absolute_error: 1.6179\n",
      "Mean squared error: 6.10\n",
      "Variance: 0.92\n",
      "Kappa Score: 0.96\n",
      "\n",
      "------------Fold 3------------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 7s 698us/step - loss: 62.7394 - mean_absolute_error: 4.3031\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 5s 490us/step - loss: 38.7938 - mean_absolute_error: 3.4874\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 5s 489us/step - loss: 33.0953 - mean_absolute_error: 3.4083\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 31.4491 - mean_absolute_error: 3.3917\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 29.3546 - mean_absolute_error: 3.2938\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 28.7074 - mean_absolute_error: 3.2280\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 5s 520us/step - loss: 27.4016 - mean_absolute_error: 3.1007\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 5s 502us/step - loss: 25.0976 - mean_absolute_error: 2.9174\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 5s 491us/step - loss: 21.7981 - mean_absolute_error: 2.7306\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 5s 493us/step - loss: 19.9643 - mean_absolute_error: 2.6122\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 18.1147 - mean_absolute_error: 2.4797\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 5s 493us/step - loss: 17.3357 - mean_absolute_error: 2.3966\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 5s 478us/step - loss: 15.8494 - mean_absolute_error: 2.2788\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 5s 482us/step - loss: 15.6009 - mean_absolute_error: 2.2683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 5s 464us/step - loss: 15.1738 - mean_absolute_error: 2.2102\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 5s 463us/step - loss: 14.3682 - mean_absolute_error: 2.1598\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 5s 453us/step - loss: 13.2707 - mean_absolute_error: 2.0810\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 5s 454us/step - loss: 13.7793 - mean_absolute_error: 2.0955\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 5s 453us/step - loss: 12.2840 - mean_absolute_error: 2.0023\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 5s 465us/step - loss: 12.1872 - mean_absolute_error: 1.9802\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 5s 456us/step - loss: 11.3628 - mean_absolute_error: 1.9252\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 5s 464us/step - loss: 11.9368 - mean_absolute_error: 1.9374\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 5s 481us/step - loss: 11.0588 - mean_absolute_error: 1.8732\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 5s 456us/step - loss: 10.5762 - mean_absolute_error: 1.8390\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 5s 457us/step - loss: 10.6009 - mean_absolute_error: 1.8194\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 5s 456us/step - loss: 10.1281 - mean_absolute_error: 1.7890\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 5s 459us/step - loss: 10.2644 - mean_absolute_error: 1.7830\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 5s 460us/step - loss: 10.5404 - mean_absolute_error: 1.8017\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 5s 458us/step - loss: 10.2631 - mean_absolute_error: 1.7629\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 5s 458us/step - loss: 10.0461 - mean_absolute_error: 1.7573\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 5s 460us/step - loss: 9.8492 - mean_absolute_error: 1.7553\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 5s 461us/step - loss: 9.3553 - mean_absolute_error: 1.7265\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 5s 511us/step - loss: 10.0202 - mean_absolute_error: 1.7627\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 5s 502us/step - loss: 8.9224 - mean_absolute_error: 1.7080\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 5s 487us/step - loss: 9.1684 - mean_absolute_error: 1.6928\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 5s 489us/step - loss: 9.3159 - mean_absolute_error: 1.7016\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 5s 508us/step - loss: 9.1109 - mean_absolute_error: 1.6773\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 5s 510us/step - loss: 9.3155 - mean_absolute_error: 1.6840\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 5s 505us/step - loss: 9.0831 - mean_absolute_error: 1.6646\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 5s 480us/step - loss: 9.2058 - mean_absolute_error: 1.6579\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 5s 482us/step - loss: 8.8510 - mean_absolute_error: 1.6604\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 5s 480us/step - loss: 8.8236 - mean_absolute_error: 1.6517\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 5s 485us/step - loss: 8.5510 - mean_absolute_error: 1.6204\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 5s 479us/step - loss: 8.7241 - mean_absolute_error: 1.6418\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 5s 483us/step - loss: 8.1650 - mean_absolute_error: 1.6121\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 5s 518us/step - loss: 8.7490 - mean_absolute_error: 1.6331\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 5s 490us/step - loss: 8.9987 - mean_absolute_error: 1.6357\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 5s 524us/step - loss: 8.3421 - mean_absolute_error: 1.5967\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 5s 489us/step - loss: 8.2987 - mean_absolute_error: 1.5912\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 8.6500 - mean_absolute_error: 1.6021\n",
      "Mean squared error: 6.62\n",
      "Variance: 0.92\n",
      "Kappa Score: 0.96\n",
      "\n",
      "------------Fold 4------------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 7s 632us/step - loss: 61.8062 - mean_absolute_error: 4.2415\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 5s 434us/step - loss: 38.0647 - mean_absolute_error: 3.4276\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 5s 512us/step - loss: 32.3431 - mean_absolute_error: 3.3331\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 6s 555us/step - loss: 29.4877 - mean_absolute_error: 3.2830\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 6s 549us/step - loss: 28.4429 - mean_absolute_error: 3.2268\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 5s 492us/step - loss: 26.7065 - mean_absolute_error: 3.1264\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 5s 481us/step - loss: 25.9373 - mean_absolute_error: 3.0163\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 5s 518us/step - loss: 23.9768 - mean_absolute_error: 2.8665\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 5s 502us/step - loss: 20.8368 - mean_absolute_error: 2.6650\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 18.4377 - mean_absolute_error: 2.4985\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 16.6688 - mean_absolute_error: 2.3544\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 15.0755 - mean_absolute_error: 2.2683\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 14.4126 - mean_absolute_error: 2.1878\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 13.3154 - mean_absolute_error: 2.1393\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 13.3313 - mean_absolute_error: 2.0917\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 5s 501us/step - loss: 12.6177 - mean_absolute_error: 2.0710\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 12.1755 - mean_absolute_error: 2.0215\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 11.6056 - mean_absolute_error: 1.9678\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 12.0152 - mean_absolute_error: 1.9823\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 11.2504 - mean_absolute_error: 1.9238\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 5s 501us/step - loss: 10.9651 - mean_absolute_error: 1.8893\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 11.1613 - mean_absolute_error: 1.8895\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 5s 506us/step - loss: 10.4603 - mean_absolute_error: 1.8369\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 10.2335 - mean_absolute_error: 1.8147\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10381/10381 [==============================] - 5s 495us/step - loss: 10.7070 - mean_absolute_error: 1.8209\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 5s 492us/step - loss: 9.9841 - mean_absolute_error: 1.7700\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 9.3481 - mean_absolute_error: 1.7321\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 5s 493us/step - loss: 8.9707 - mean_absolute_error: 1.7036\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 5s 505us/step - loss: 9.1233 - mean_absolute_error: 1.7122\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 5s 493us/step - loss: 9.0131 - mean_absolute_error: 1.6878\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 5s 505us/step - loss: 8.8797 - mean_absolute_error: 1.6654\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 9.1904 - mean_absolute_error: 1.6849\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 8.9931 - mean_absolute_error: 1.6808\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 5s 517us/step - loss: 8.8238 - mean_absolute_error: 1.6574\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 6s 559us/step - loss: 8.8727 - mean_absolute_error: 1.6693\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 6s 592us/step - loss: 8.9476 - mean_absolute_error: 1.6678\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 5s 510us/step - loss: 8.6877 - mean_absolute_error: 1.6557\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 5s 494us/step - loss: 8.4947 - mean_absolute_error: 1.6284\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 5s 492us/step - loss: 8.6098 - mean_absolute_error: 1.6363\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 9.0927 - mean_absolute_error: 1.6535\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 5s 494us/step - loss: 8.6131 - mean_absolute_error: 1.6341\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 8.6065 - mean_absolute_error: 1.6199\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 5s 489us/step - loss: 8.2661 - mean_absolute_error: 1.5964\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 8.4152 - mean_absolute_error: 1.6020\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 8.2218 - mean_absolute_error: 1.5985\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 8.4344 - mean_absolute_error: 1.6065\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 7.9875 - mean_absolute_error: 1.5602\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 8.3484 - mean_absolute_error: 1.5749\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 6s 580us/step - loss: 8.3756 - mean_absolute_error: 1.5815\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 6s 534us/step - loss: 7.8718 - mean_absolute_error: 1.5616\n",
      "Mean squared error: 6.28\n",
      "Variance: 0.93\n",
      "Kappa Score: 0.96\n",
      "\n",
      "------------Fold 5------------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 8s 816us/step - loss: 63.3462 - mean_absolute_error: 4.3520\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 5s 491us/step - loss: 38.5064 - mean_absolute_error: 3.4534\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 32.5399 - mean_absolute_error: 3.4217\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 29.9343 - mean_absolute_error: 3.3569\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 28.7590 - mean_absolute_error: 3.3170\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 28.1196 - mean_absolute_error: 3.2509\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 5s 506us/step - loss: 26.8361 - mean_absolute_error: 3.1492\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 5s 494us/step - loss: 25.9586 - mean_absolute_error: 3.0139\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 23.6403 - mean_absolute_error: 2.8379\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 5s 501us/step - loss: 21.0886 - mean_absolute_error: 2.6845\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 18.8490 - mean_absolute_error: 2.4905\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 16.7084 - mean_absolute_error: 2.3526\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 16.0753 - mean_absolute_error: 2.3059\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 5s 493us/step - loss: 15.8031 - mean_absolute_error: 2.2835\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 5s 491us/step - loss: 14.3307 - mean_absolute_error: 2.1801\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 14.0177 - mean_absolute_error: 2.1563\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 13.0509 - mean_absolute_error: 2.0860\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 5s 505us/step - loss: 13.3083 - mean_absolute_error: 2.0789\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 5s 492us/step - loss: 12.8418 - mean_absolute_error: 2.0306\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 5s 492us/step - loss: 12.6497 - mean_absolute_error: 1.9933\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 12.1005 - mean_absolute_error: 1.9664\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 5s 494us/step - loss: 11.7855 - mean_absolute_error: 1.9257\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 11.4067 - mean_absolute_error: 1.8914\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 11.1301 - mean_absolute_error: 1.8890\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 5s 494us/step - loss: 11.0395 - mean_absolute_error: 1.8666\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 10.6528 - mean_absolute_error: 1.8443\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 5s 493us/step - loss: 10.2159 - mean_absolute_error: 1.7952\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 10.5157 - mean_absolute_error: 1.8110\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 10.1168 - mean_absolute_error: 1.7925\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 9.8121 - mean_absolute_error: 1.7509\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 9.7297 - mean_absolute_error: 1.7381\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 9.6350 - mean_absolute_error: 1.7428\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 5s 493us/step - loss: 9.4008 - mean_absolute_error: 1.6869\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 5s 493us/step - loss: 9.2283 - mean_absolute_error: 1.6991\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 5s 493us/step - loss: 9.4999 - mean_absolute_error: 1.6964\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10381/10381 [==============================] - 5s 505us/step - loss: 9.3196 - mean_absolute_error: 1.6803\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 9.2886 - mean_absolute_error: 1.6820\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 9.1376 - mean_absolute_error: 1.6636\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 9.3697 - mean_absolute_error: 1.6880\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 9.0060 - mean_absolute_error: 1.6643\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 9.0889 - mean_absolute_error: 1.6602\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 9.3453 - mean_absolute_error: 1.6643\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 8.8430 - mean_absolute_error: 1.6414\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 5s 493us/step - loss: 9.3011 - mean_absolute_error: 1.6449\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 5s 493us/step - loss: 9.1389 - mean_absolute_error: 1.6386\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 8.9709 - mean_absolute_error: 1.6339\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 5s 494us/step - loss: 8.4530 - mean_absolute_error: 1.6068\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 5s 505us/step - loss: 8.3259 - mean_absolute_error: 1.5938\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 5s 494us/step - loss: 8.5853 - mean_absolute_error: 1.5904\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 5s 493us/step - loss: 8.5987 - mean_absolute_error: 1.6165\n",
      "Mean squared error: 6.24\n",
      "Variance: 0.92\n",
      "Kappa Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Applying k-fold cross validation\n",
    "\n",
    "cv = KFold(len(dataset), n_folds=5, shuffle=True)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv:\n",
    "    print(\"\\n------------Fold {}------------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = dataset.iloc[testcv], dataset.iloc[traincv], scores.iloc[testcv], scores.iloc[traincv]\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "            # Obtaining all sentences from the training set of essays.\n",
    "            sentences += sentence_tokens(essay)\n",
    "            \n",
    "    # Initializing variables for word2vec model.\n",
    "    num_features = 300 \n",
    "    min_word_count = 40\n",
    "    num_workers = 4\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "    clean_train_essays = []\n",
    "    \n",
    "    # Generate training and testing data word vectors.\n",
    "    for essay_text in train_essays:\n",
    "        clean_train_essays.append(word_tokens(essay_text))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    for essay_text in test_essays:\n",
    "        clean_test_essays.append(word_tokens(essay_text))\n",
    "    testDataVecs = getAvgFeatureVecs(clean_test_essays, model, num_features)\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "    \n",
    "    lstm_model = get_model()\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
    "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.around(y_pred)\n",
    "    \n",
    "    '''Evaluation metric used : \n",
    "    1. Mean squared error\n",
    "    2. Variance\n",
    "    3. Cohen's kappa score\n",
    "    Expected results - Minimum error, maximum variance(For variance, best possible score is 1.0, lower \n",
    "    values are worse.) and maximum kappa score(1 depicting the best scores)'''\n",
    "    \n",
    "    # Mean squared error\n",
    "    print(\"Mean squared error: {0:.2f}\".format(mean_squared_error(y_test.values, y_pred)))\n",
    "\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance: {0:.2f}'.format(explained_variance_score(y_test.values, y_pred)))  \n",
    "    \n",
    "    #Cohen's kappa score\n",
    "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "    print(\"Kappa Score: {0:.2f}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Kappa score after a 5-fold cross validation:  0.96\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux-device/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset into training and test set and generating word embeddings for other models other than\n",
    "# neural networks\n",
    "\n",
    "indep_train, indep_test, dep_train, dep_test = train_test_split(dataset, scores, test_size = 0.25)\n",
    "\n",
    "train_essays2 = indep_train['essay']\n",
    "test_essays2 = indep_test['essay']\n",
    "    \n",
    "sentences2 = []\n",
    "\n",
    "\n",
    "for essay2 in train_essays2:\n",
    "            # Obtaining all sentences from the training set of essays.\n",
    "            sentences2 += sentence_tokens(essay2)\n",
    "            \n",
    "# Initializing variables for word2vec model.\n",
    "num_features = 300 \n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "print(\"Training Word2Vec Model...\")\n",
    "model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "clean_train_essays2 = []\n",
    "    \n",
    "# Generate training and testing data word vectors.\n",
    "for essay_text2 in train_essays2:\n",
    "    clean_train_essays2.append(word_tokens(essay_text2))\n",
    "trainDataVecs2 = getAvgFeatureVecs(clean_train_essays2, model, num_features)\n",
    "    \n",
    "clean_test_essays2 = []\n",
    "for essay_text2 in test_essays2:\n",
    "    clean_test_essays2.append(word_tokens(essay_text2))\n",
    "testDataVecs2 = getAvgFeatureVecs(clean_test_essays2, model, num_features)\n",
    "    \n",
    "trainDataVecs2 = np.array(trainDataVecs2)\n",
    "testDataVecs2 = np.array(testDataVecs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 20.29\n",
      "Variance score: 0.75\n",
      "Kappa Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Generating scores using Linear Regression Model\n",
    "\n",
    "linear_regressor = LinearRegression()\n",
    "\n",
    "linear_regressor.fit(trainDataVecs2, dep_train)\n",
    "\n",
    "dep_pred = linear_regressor.predict(testDataVecs2)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(dep_test, dep_pred))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % explained_variance_score(dep_test,dep_pred))\n",
    "\n",
    "#print('Cohen\\'s kappa score: %.2f' % cohen_kappa_score(dep_pred, dep_test))\n",
    "print(\"Kappa Score: {0:.2f}\".format(cohen_kappa_score(dep_test.values,np.around(dep_pred),weights='quadratic')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 9.58\n",
      "Variance score: 0.88\n",
      "Kappa Score: 0.94\n"
     ]
    }
   ],
   "source": [
    "#Generating scores using Gradient Boosting regressor\n",
    "\n",
    "'''from sklearn.model_selection import GridSearchCV\n",
    "params = {'n_estimators':[100, 1000], 'max_depth':[2], 'min_samples_split': [2],\n",
    "          'learning_rate':[3, 1, 0.1, 0.3], 'loss': ['ls']}\n",
    "\n",
    "gbr = ensemble.GradientBoostingRegressor()\n",
    "\n",
    "grid = GridSearchCV(gbr, params)\n",
    "grid.fit(trainDataVecs2, dep_train)\n",
    "\n",
    "y_pred = grid.predict(testDataVecs2)\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)'''\n",
    "\n",
    "#USING THE PARAMS FOUND OUT USING GRID SEARCH CV\n",
    "gbr = ensemble.GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
    "             learning_rate=0.1, loss='ls', max_depth=2, max_features=None,\n",
    "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "             min_impurity_split=None, min_samples_leaf=1,\n",
    "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "             n_estimators=1000, presort='auto', random_state=None,\n",
    "             subsample=1.0, verbose=0, warm_start=False)\n",
    "gbr.fit(trainDataVecs2, dep_train)\n",
    "dep_pred = gbr.predict(testDataVecs2)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(dep_test, dep_pred))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % explained_variance_score(dep_test,dep_pred))\n",
    "\n",
    "print(\"Kappa Score: {0:.2f}\".format(cohen_kappa_score(dep_test.values,np.around(dep_pred),weights='quadratic')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 41.51\n",
      "Variance score: 0.52\n",
      "Kappa Score: 0.64\n"
     ]
    }
   ],
   "source": [
    "svr = SVR()\n",
    "\n",
    "'''parameters = {'kernel':['linear', 'rbf'], 'C':[1, 100], 'gamma':[0.1, 0.001]}\n",
    "\n",
    "grid = GridSearchCV(svr, parameters)\n",
    "grid.fit(trainDataVecs2, dep_train)\n",
    "\n",
    "y_pred = grid.predict(testDataVecs2)\n",
    "\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)'''\n",
    "\n",
    "#USING THE PARAMS FOUND OUT USING GRID SEARCH CV\n",
    "svr = SVR(C=100, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "svr.fit(trainDataVecs2, dep_train)\n",
    "dep_pred = svr.predict(testDataVecs2)\n",
    "\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(dep_test, dep_pred))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % explained_variance_score(dep_test,dep_pred))\n",
    "\n",
    "#Cohen's Kappa score\n",
    "print(\"Kappa Score: {0:.2f}\".format(cohen_kappa_score(dep_test.values,np.around(dep_pred),weights='quadratic')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As lstm outperforms all other models, so using it for predicting the scores for the final dataset\n",
    "valid_set = pd.read_csv('valid_set.tsv', sep='\\t', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set = valid_set.drop(['domain2_predictionid'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_predictionid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1788</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @ORGANIZATION1, @CAPS1 more and more peop...</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1789</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1 Time @CAPS1 me tell you what I...</td>\n",
       "      <td>1789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1790</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local newspaper, Have you been spending a...</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1791</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Readers, @CAPS1 you imagine how life woul...</td>\n",
       "      <td>1791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1792</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear newspaper, I strongly believe that comput...</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0      1788          1  Dear @ORGANIZATION1, @CAPS1 more and more peop...   \n",
       "1      1789          1  Dear @LOCATION1 Time @CAPS1 me tell you what I...   \n",
       "2      1790          1  Dear Local newspaper, Have you been spending a...   \n",
       "3      1791          1  Dear Readers, @CAPS1 you imagine how life woul...   \n",
       "4      1792          1  Dear newspaper, I strongly believe that comput...   \n",
       "\n",
       "   domain1_predictionid  \n",
       "0                  1788  \n",
       "1                  1789  \n",
       "2                  1790  \n",
       "3                  1791  \n",
       "4                  1792  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_test_essays = valid_set['essay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Dear @ORGANIZATION1, @CAPS1 more and more peop...\n",
       "1       Dear @LOCATION1 Time @CAPS1 me tell you what I...\n",
       "2       Dear Local newspaper, Have you been spending a...\n",
       "3       Dear Readers, @CAPS1 you imagine how life woul...\n",
       "4       Dear newspaper, I strongly believe that comput...\n",
       "5       Dear local newspaper, @CAPS1 the caveman found...\n",
       "6       Dear newspaper editor, @CAPS1 now is a letter ...\n",
       "7       Dear @ORGANIZATION1, @CAPS1, there has been so...\n",
       "8       Dear Local Newspaper, I would like to complain...\n",
       "9       Dear Newspaper, @CAPS1 having kids wasting the...\n",
       "10      Dear @CAPS1 @CAPS2, @CAPS3 should think that c...\n",
       "11      Dear Local @CAPS1, Computers, and other techno...\n",
       "12      Why people should use computers? Computers are...\n",
       "13      @CAPS1, its time for supper! @CAPS2 on, mom I ...\n",
       "14      Dear local newspaper: I am writing this letter...\n",
       "15      Effects of computers @CAPS1 local newspaper, H...\n",
       "16      Dear to whom it @MONTH1 concern, @CAPS1 is an ...\n",
       "17      Dear @ORGANIZATION1, Advances in technology ca...\n",
       "18      Dear newspaper @CAPS1, I am writing this lette...\n",
       "19      Dear Local newspaper, I do agree that to many ...\n",
       "20      Dear Local Newspaper, I heard about people's o...\n",
       "21      Dear @CAPS1, I believe that computers have an ...\n",
       "22      Although some people believe computers aren't ...\n",
       "23      Computors are looked at as either friend or fo...\n",
       "24      Dear local Newspaper, I am writing to you toda...\n",
       "25      Dear news paper, Computers are good for people...\n",
       "26      Dear @CAPS1 @CAPS2 Newspaper, Have you ever wa...\n",
       "27      Dear Newspaper. I @CAPS2 that people are spend...\n",
       "28      Dear @ORGANIZATION1, Computers have bad effect...\n",
       "29      Dear @CAPS1, @CAPS2 name is @CAPS3 @CAPS4. I g...\n",
       "                              ...                        \n",
       "4188    Just @CAPS1 @CAPS2 @DATE1 is @LOCATION2's @CAP...\n",
       "4189    Dad's @CAPS1 In the not so distant past I need...\n",
       "4190     It's that amazing feeling in your core, where...\n",
       "4191     @PERSON1 sits at a table, working on an art p...\n",
       "4192       Have you ever been furious enough to cry ? ...\n",
       "4193     Once when I was on my way over to my best fri...\n",
       "4194     My mom and I have a unique relationship. She ...\n",
       "4195     This test is a bunch of crap and its funny th...\n",
       "4196     Working at the local glass factory is not nec...\n",
       "4197    The @ORGANIZATION2 and I @CAPS1 to the @CAPS2 ...\n",
       "4198     Over this last @DATE1 some friends and I were...\n",
       "4199       Have you ever had a really bad day and noth...\n",
       "4200                                    LaughterLaught...\n",
       "4201    Laughter is a tool that every one knows how to...\n",
       "4202     Laughter is a good part of life. It demonstra...\n",
       "4203    It all started off on a cold sunny @DATE1 @TIM...\n",
       "4204    There have been many times in my life that I h...\n",
       "4205     The sunset shimmers off of the sparkling wate...\n",
       "4206      Laughter has played a significant role in my...\n",
       "4207     In the @DATE1 ,my best friend @PERSON1 and I ...\n",
       "4208      My friends and I always have a great time to...\n",
       "4209      Everybody laughs in their life time, sometim...\n",
       "4210     Laughter to me is important in any relationsh...\n",
       "4211     I believe that laughter is a huge part of any...\n",
       "4212                           laughter is the best me...\n",
       "4213     Have you ever noticed that if two little kids...\n",
       "4214                                Laughter @CAPS1 I ...\n",
       "4215     Laughter in @CAPS1 A laugh is not just an act...\n",
       "4216      LAUGHTER @CAPS1 i was younger my friend live...\n",
       "4217     You know how the saying goes live, laugh, lov...\n",
       "Name: essay, Length: 4218, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_test_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux-device/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/linux-device/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "    \n",
    "for valid_essay in valid_test_essays:\n",
    "        sentences += sentence_tokens(valid_essay)\n",
    "            \n",
    "num_features = 300 \n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "print(\"Training Word2Vec Model...\")\n",
    "model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "valid_clean_test_essays = []\n",
    "    \n",
    "# Generate training and testing data word vectors.\n",
    "for essay_text in valid_test_essays:\n",
    "    valid_clean_test_essays.append(word_tokens(essay_text))\n",
    "valid_testDataVecs = getAvgFeatureVecs(valid_clean_test_essays, model, num_features)\n",
    "\n",
    "valid_testDataVecs = np.array(valid_testDataVecs)\n",
    "# Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "valid_testDataVecs = np.reshape(valid_testDataVecs, (valid_testDataVecs.shape[0], 1, valid_testDataVecs.shape[1]))\n",
    "    \n",
    "predicted_scores = lstm_model.predict(valid_testDataVecs)\n",
    "    \n",
    "# Round y_pred to the nearest integer.\n",
    "predicted_scores = np.around(predicted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = valid_set.drop(['essay'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_score = predicted_scores.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_score = pd.Series([score for sublist in predicted_scores for score in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6.0\n",
       "1    5.0\n",
       "2    7.0\n",
       "3    7.0\n",
       "4    6.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_score.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([submission, predicted_score], axis = 1).rename(columns = {0:\"predicted_score\"}).iloc[:,[2,0,1,3]]\n",
    "submission.to_excel(\"Submission.xls\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
